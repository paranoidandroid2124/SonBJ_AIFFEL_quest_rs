{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ab4dc64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b69ecc77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3b49372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì™„ë£Œ!\n"
     ]
    }
   ],
   "source": [
    "%config InlineBackend.figure_format = 'retina'\n",
    " \n",
    "import matplotlib.font_manager as fm\n",
    "fontpath = '/usr/share/fonts/truetype/nanum/NanumBarunGothic.ttf'\n",
    "font = fm.FontProperties(fname=fontpath, size=9)\n",
    "plt.rc('font', family='NanumBarunGothic') \n",
    "mpl.font_manager.findfont(font)\n",
    "\n",
    "print(\"ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90129deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# enc_train_data = 'korean-english-park.train.ko'\n",
    "# dec_train_data = 'korean-english-park.train.en'\n",
    "# enc_val_data = 'korean-english-park.dev.ko'\n",
    "# dec_val_data = 'korean-english-park.dev.en'\n",
    "# enc_test_data = 'korean-english-park.test.ko'\n",
    "# dec_test_data = 'korean-english-park.test.en'\n",
    "\n",
    "# # ë°ì´í„° ì½ê¸°\n",
    "# def load_data(file_path):\n",
    "#     with open(file_path, 'r', encoding='utf-8') as file:\n",
    "#         data = file.readlines()\n",
    "#     return data\n",
    "\n",
    "# # íŒŒì¼ ì½ê¸°\n",
    "# enc_train_data = load_data(enc_train_data)\n",
    "# dec_train_data = load_data(dec_train_data)\n",
    "# enc_val_data = load_data(enc_val_data)\n",
    "# dec_val_data = load_data(dec_val_data)\n",
    "# enc_test_data = load_data(enc_test_data)\n",
    "# dec_test_data = load_data(dec_test_data)\n",
    "\n",
    "# # ë°ì´í„° ë¯¸ë¦¬ ë³´ê¸°\n",
    "# print(enc_train_data[:3])  # ì²« 3ê°œ í•­ëª© í™•ì¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e202040c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# enc_data = enc_train_data + enc_val_data\n",
    "# dec_data = dec_train_data + dec_val_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "13e96881",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len(enc_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0bd0b888",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len(enc_val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "70bd99ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # íŠ¹ìˆ˜ê¸°í˜¸ í™•ì¸í•˜ëŠ” í•¨ìˆ˜\n",
    "# def find_special_char(data):\n",
    "#     # í•œê¸€ê³¼ ìˆ«ìë¥¼ ì œì™¸í•œ íŠ¹ìˆ˜ë¬¸ìë§Œ ì°¾ëŠ” ì •ê·œí‘œí˜„ì‹\n",
    "#     pattern = r'[^ê°€-í£0-9a-zA-Z\\s]'\n",
    "    \n",
    "#     special_chars = []\n",
    "    \n",
    "#     # ë¦¬ìŠ¤íŠ¸ì˜ ê° í•­ëª©ì— ëŒ€í•´ íŠ¹ìˆ˜ê¸°í˜¸ë¥¼ ì°¾ìŒ\n",
    "#     for text in data:\n",
    "#         if isinstance(text, str):  # ë¬¸ìì—´ì¸ ê²½ìš°ì—ë§Œ ì²˜ë¦¬\n",
    "#             # ì •ê·œí‘œí˜„ì‹ì„ í†µí•´ íŠ¹ìˆ˜ë¬¸ì ì¶”ì¶œ\n",
    "#             special_chars.extend(re.findall(pattern, text))\n",
    "    \n",
    "#     return special_chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "776d7826",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'!',\n",
       " '\"',\n",
       " '#',\n",
       " '$',\n",
       " '%',\n",
       " '&',\n",
       " \"'\",\n",
       " '(',\n",
       " ')',\n",
       " '*',\n",
       " '+',\n",
       " ',',\n",
       " '-',\n",
       " '.',\n",
       " '/',\n",
       " ':',\n",
       " ';',\n",
       " '=',\n",
       " '>',\n",
       " '?',\n",
       " '@',\n",
       " '[',\n",
       " ']',\n",
       " '^',\n",
       " '_',\n",
       " '`',\n",
       " '{',\n",
       " '}',\n",
       " '~',\n",
       " 'Â±',\n",
       " 'Â²',\n",
       " 'Â´',\n",
       " 'Â·',\n",
       " 'Ã—',\n",
       " 'Ã©',\n",
       " 'Ë™',\n",
       " 'Ë',\n",
       " 'â€“',\n",
       " 'â€•',\n",
       " 'â€˜',\n",
       " 'â€™',\n",
       " 'â€œ',\n",
       " 'â€',\n",
       " 'â€¢',\n",
       " 'â€¥',\n",
       " 'â€¦',\n",
       " 'â„ƒ',\n",
       " 'â„“',\n",
       " 'â–²',\n",
       " 'â–¶',\n",
       " 'ã€ˆ',\n",
       " 'ã€‰',\n",
       " 'ã€”',\n",
       " 'ã€•',\n",
       " 'ã„',\n",
       " 'ã‹',\n",
       " 'ã',\n",
       " 'ã',\n",
       " 'ã•',\n",
       " 'ã–',\n",
       " 'ã—',\n",
       " 'ã™',\n",
       " 'ã',\n",
       " 'ã¦',\n",
       " 'ã¨',\n",
       " 'ã°',\n",
       " 'ã¶',\n",
       " 'ã·',\n",
       " 'ã¾',\n",
       " 'ã‚„',\n",
       " 'ã‚ˆ',\n",
       " 'ã‚‰',\n",
       " 'ã‚Š',\n",
       " 'ã‚“',\n",
       " 'ã„´',\n",
       " 'ã…‡',\n",
       " 'ã…‹',\n",
       " 'ã†',\n",
       " 'ãˆœ',\n",
       " 'ã',\n",
       " 'ã',\n",
       " 'ã ',\n",
       " 'ã¡',\n",
       " 'ã¢',\n",
       " 'ã¾',\n",
       " 'ä¸€',\n",
       " 'ä¸‡',\n",
       " 'ä¸‰',\n",
       " 'ä¸Š',\n",
       " 'ä¸',\n",
       " 'ä¸™',\n",
       " 'ä¸­',\n",
       " 'ä¸»',\n",
       " 'ä¹‹',\n",
       " 'ä¹',\n",
       " 'äº',\n",
       " 'äº¤',\n",
       " 'äº¬',\n",
       " 'ä»',\n",
       " 'ä»£',\n",
       " 'ä½›',\n",
       " 'ä¿ƒ',\n",
       " 'ä¿¡',\n",
       " 'ä¿®',\n",
       " 'å‡',\n",
       " 'å‚™',\n",
       " 'å‚·',\n",
       " 'åƒ',\n",
       " 'åƒ',\n",
       " 'åƒ¹',\n",
       " 'å„„',\n",
       " 'å…†',\n",
       " 'å…‰',\n",
       " 'å…‹',\n",
       " 'å…’',\n",
       " 'å…§',\n",
       " 'å…¨',\n",
       " 'å…¬',\n",
       " 'å…µ',\n",
       " 'å†',\n",
       " 'åˆ‡',\n",
       " 'å‰',\n",
       " 'åŠ‰',\n",
       " 'åŠ›',\n",
       " 'åŠ ',\n",
       " 'å‹•',\n",
       " 'å‹',\n",
       " 'åŒ—',\n",
       " 'å€',\n",
       " 'å',\n",
       " 'å ',\n",
       " 'å°',\n",
       " 'å¿',\n",
       " 'å',\n",
       " 'å—',\n",
       " 'å¤',\n",
       " 'å¸',\n",
       " 'å‰',\n",
       " 'åŒ',\n",
       " 'å',\n",
       " 'å³',\n",
       " 'å¾',\n",
       " 'å‘½',\n",
       " 'å’Œ',\n",
       " 'å“',\n",
       " 'å•†',\n",
       " 'å–„',\n",
       " 'å–©',\n",
       " 'å™¨',\n",
       " 'å››',\n",
       " 'å› ',\n",
       " 'åœ‹',\n",
       " 'åœ’',\n",
       " 'åœ“',\n",
       " 'åœ˜',\n",
       " 'åœ¨',\n",
       " 'åœ°',\n",
       " 'å¤',\n",
       " 'å‹',\n",
       " 'åŸ',\n",
       " 'åŸº',\n",
       " 'å €',\n",
       " 'å ±',\n",
       " 'å ´',\n",
       " 'å¡”',\n",
       " 'å¢“',\n",
       " 'å£½',\n",
       " 'å¤–',\n",
       " 'å¤š',\n",
       " 'å¤§',\n",
       " 'å¤©',\n",
       " 'å¤«',\n",
       " 'å¥”',\n",
       " 'å¥´',\n",
       " 'å§“',\n",
       " 'å©¦',\n",
       " 'å­',\n",
       " 'å­—',\n",
       " 'å­©',\n",
       " 'å­¸',\n",
       " 'å®‡',\n",
       " 'å®‰',\n",
       " 'å®˜',\n",
       " 'å®¤',\n",
       " 'å®¶',\n",
       " 'å¯Œ',\n",
       " 'å¯Ÿ',\n",
       " 'å¯§',\n",
       " 'å¯¶',\n",
       " 'å¯º',\n",
       " 'å°',\n",
       " 'å°',\n",
       " 'å°',\n",
       " 'å±€',\n",
       " 'å±…',\n",
       " 'å±‹',\n",
       " 'å±±',\n",
       " 'å²©',\n",
       " 'å³¶',\n",
       " 'å³½',\n",
       " 'å·',\n",
       " 'å·',\n",
       " 'å¸‚',\n",
       " 'å¸­',\n",
       " 'å¸¸',\n",
       " 'å¹³',\n",
       " 'å¹¹',\n",
       " 'åº—',\n",
       " 'åº¦',\n",
       " 'åº§',\n",
       " 'åº«',\n",
       " 'åº·',\n",
       " 'å»£',\n",
       " 'å»º',\n",
       " 'å¼',\n",
       " 'å¼—',\n",
       " 'å¼º',\n",
       " 'å½±',\n",
       " 'å¾Œ',\n",
       " 'å¿ƒ',\n",
       " 'æ€§',\n",
       " 'æ„',\n",
       " 'æ„š',\n",
       " 'æ„›',\n",
       " 'æ…¶',\n",
       " 'æ‡·',\n",
       " 'æˆ°',\n",
       " 'æ‰€',\n",
       " 'æ‰‹',\n",
       " 'æƒ',\n",
       " 'æŒ',\n",
       " 'æ›',\n",
       " 'æ’­',\n",
       " 'æ•…',\n",
       " 'æ•ˆ',\n",
       " 'æ•',\n",
       " 'æ•¢',\n",
       " 'æ•µ',\n",
       " 'æ–‡',\n",
       " 'æ–Œ',\n",
       " 'æ–°',\n",
       " 'æ–¹',\n",
       " 'æ—',\n",
       " 'æ——',\n",
       " 'æ—¥',\n",
       " 'æ—©',\n",
       " 'æ˜',\n",
       " 'æ˜Ÿ',\n",
       " 'æ™‚',\n",
       " 'æ™‰',\n",
       " 'æ™´',\n",
       " 'æ›¸',\n",
       " 'æ›¾',\n",
       " 'æœ€',\n",
       " 'æœƒ',\n",
       " 'æœˆ',\n",
       " 'æœ¬',\n",
       " 'æœ±',\n",
       " 'æ',\n",
       " 'æ±',\n",
       " 'æ—',\n",
       " 'æŸ»',\n",
       " 'æ ª',\n",
       " 'æ¡ˆ',\n",
       " 'æ£º',\n",
       " 'æ¤',\n",
       " 'æ¥Š',\n",
       " 'æ©Ÿ',\n",
       " 'æ¬Š',\n",
       " 'æ¬¡',\n",
       " 'æ­¢',\n",
       " 'æ­£',\n",
       " 'æ­»',\n",
       " 'æ®˜',\n",
       " 'æ¯',\n",
       " 'æ¯',\n",
       " 'æ¯›',\n",
       " 'æ¯¬',\n",
       " 'æ°‘',\n",
       " 'æ°£',\n",
       " 'æ°´',\n",
       " 'æ°·',\n",
       " 'æ±Ÿ',\n",
       " 'æ²ˆ',\n",
       " 'æ²³',\n",
       " 'æ²¹',\n",
       " 'æ³•',\n",
       " 'æ³¥',\n",
       " 'æ³°',\n",
       " 'æµ',\n",
       " 'æµ…',\n",
       " 'æµ·',\n",
       " 'æ·˜',\n",
       " 'æ·º',\n",
       " 'æ¸¯',\n",
       " 'æ¹–',\n",
       " 'æ¹›',\n",
       " 'æº«',\n",
       " 'æ»‘',\n",
       " 'æ¾¤',\n",
       " 'æ¿ ',\n",
       " 'æ¿¤',\n",
       " 'æ¿±',\n",
       " 'ç€‹',\n",
       " 'ç£',\n",
       " 'ç«',\n",
       " 'ç„¡',\n",
       " 'ç„¶',\n",
       " 'ç„¼',\n",
       " 'ç…™',\n",
       " 'ç…§',\n",
       " 'ç‡­',\n",
       " 'çˆ¶',\n",
       " 'çˆ¾',\n",
       " 'ç‰©',\n",
       " 'ç¨',\n",
       " 'ç¸',\n",
       " 'ç‡',\n",
       " 'ç‹',\n",
       " 'ç€',\n",
       " 'ç¾',\n",
       " 'çƒ',\n",
       " 'ç†',\n",
       " 'ç¥',\n",
       " 'ç’°',\n",
       " 'ç”Ÿ',\n",
       " 'ç”£',\n",
       " 'ç”°',\n",
       " 'ç”·',\n",
       " 'ç•°',\n",
       " 'ç•µ',\n",
       " 'ç–†',\n",
       " 'ç–‘',\n",
       " 'ç–«',\n",
       " 'ç—…',\n",
       " 'ç™»',\n",
       " 'ç™¼',\n",
       " 'çš„',\n",
       " 'çœ',\n",
       " 'çœ',\n",
       " 'çŸ³',\n",
       " 'ç¤¾',\n",
       " 'ç¥',\n",
       " 'ç¥¨',\n",
       " 'ç¥­',\n",
       " 'ç¦',\n",
       " 'ç§',\n",
       " 'ç¨®',\n",
       " 'ç©º',\n",
       " 'çª',\n",
       " 'ç« ',\n",
       " 'ç®¡',\n",
       " 'ç¯€',\n",
       " 'ç¯‰',\n",
       " 'ç²¾',\n",
       " 'ç´…',\n",
       " 'ç´™',\n",
       " 'çµ',\n",
       " 'ç¶­',\n",
       " 'ç¸½',\n",
       " 'çº',\n",
       " 'ç½®',\n",
       " 'ç¾…',\n",
       " 'ç¾',\n",
       " 'ç¾¤',\n",
       " 'ç¿’',\n",
       " 'ç¿”',\n",
       " 'è€',\n",
       " 'è€…',\n",
       " 'è€³',\n",
       " 'è–',\n",
       " 'è²',\n",
       " 'è·',\n",
       " 'è‚¢',\n",
       " 'èƒ¡',\n",
       " 'èƒ½',\n",
       " 'è†š',\n",
       " 'è‡º',\n",
       " 'èˆŠ',\n",
       " 'èˆª',\n",
       " 'èˆ¹',\n",
       " 'èŠ±',\n",
       " 'èŠ½',\n",
       " 'è‹±',\n",
       " 'è‰',\n",
       " 'è¯',\n",
       " 'è‘£',\n",
       " 'è‘¬',\n",
       " 'è“„',\n",
       " 'è“‹',\n",
       " 'è”£',\n",
       " 'è—¥',\n",
       " 'è—»',\n",
       " 'è™Ÿ',\n",
       " 'è¡—',\n",
       " 'è¡›',\n",
       " 'è¦',\n",
       " 'è¦–',\n",
       " 'è¦ª',\n",
       " 'è¦º',\n",
       " 'è¨€',\n",
       " 'è©©',\n",
       " 'èªŒ',\n",
       " 'èª',\n",
       " 'è«‡',\n",
       " 'è«–',\n",
       " 'è«¡',\n",
       " 'è«¸',\n",
       " 'è­˜',\n",
       " 'è²§',\n",
       " 'è²·',\n",
       " 'è²»',\n",
       " 'è³£',\n",
       " 'è³ª',\n",
       " 'èµ¤',\n",
       " 'èµ°',\n",
       " 'è¶…',\n",
       " 'è¶³',\n",
       " 'è·¯',\n",
       " 'èº«',\n",
       " 'è»Œ',\n",
       " 'è»',\n",
       " 'è¼”',\n",
       " 'è¼¿',\n",
       " 'è¾›',\n",
       " 'è¿‘',\n",
       " 'è¿·',\n",
       " 'é€',\n",
       " 'é€š',\n",
       " 'é€²',\n",
       " 'é“',\n",
       " 'é‚¦',\n",
       " 'é‚¸',\n",
       " 'éƒ',\n",
       " 'éƒ¡',\n",
       " 'éƒ½',\n",
       " 'é„•',\n",
       " 'é…',\n",
       " 'é…’',\n",
       " 'é…¸',\n",
       " 'é†’',\n",
       " 'é†œ',\n",
       " 'é‡',\n",
       " 'éŠ€',\n",
       " 'éŠ…',\n",
       " 'éŒ¦',\n",
       " 'é”',\n",
       " 'é•·',\n",
       " 'é–€',\n",
       " 'é–‹',\n",
       " 'é–“',\n",
       " 'é™³',\n",
       " 'é™½',\n",
       " 'é›™',\n",
       " 'é›£',\n",
       " 'é›¨',\n",
       " 'é›²',\n",
       " 'é›»',\n",
       " 'éœ§',\n",
       " 'é‘',\n",
       " 'éœ',\n",
       " 'é',\n",
       " 'é ',\n",
       " 'é ‚',\n",
       " 'é ­',\n",
       " 'é¡Œ',\n",
       " 'é¡',\n",
       " 'é£Ÿ',\n",
       " 'é¦™',\n",
       " 'é¦¬',\n",
       " 'é§',\n",
       " 'é«”',\n",
       " 'é«®',\n",
       " 'é­š',\n",
       " 'é³¥',\n",
       " 'é³©',\n",
       " 'éº¥',\n",
       " 'éº»',\n",
       " 'é¼“',\n",
       " '\\uf0d7',\n",
       " 'ï¤Š',\n",
       " 'ï¤',\n",
       " 'ï¥§',\n",
       " 'ï¥¼',\n",
       " 'ï¦',\n",
       " 'ï¦',\n",
       " 'ï¦',\n",
       " 'ï¦´',\n",
       " 'ï¦¾',\n",
       " 'ï§ƒ',\n",
       " 'ï§„',\n",
       " 'ï§‰',\n",
       " 'ï§—',\n",
       " 'ï§›',\n",
       " 'ï§¼',\n",
       " 'ï¼',\n",
       " 'ï¼ˆ',\n",
       " 'ï¼‰',\n",
       " 'ï¼',\n",
       " 'ï¼',\n",
       " 'ï¼‘',\n",
       " 'ï¼’',\n",
       " 'ï¼“',\n",
       " 'ï¼”',\n",
       " 'ï¼•',\n",
       " 'ï¼˜',\n",
       " 'ï¼Ÿ',\n",
       " '\\U000d51d1'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set(find_special_char(enc_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2c4b5ca1",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'!',\n",
       " '\"',\n",
       " '#',\n",
       " '$',\n",
       " '%',\n",
       " '&',\n",
       " \"'\",\n",
       " '(',\n",
       " ')',\n",
       " '*',\n",
       " '+',\n",
       " ',',\n",
       " '-',\n",
       " '.',\n",
       " '/',\n",
       " ':',\n",
       " ';',\n",
       " '<',\n",
       " '=',\n",
       " '>',\n",
       " '?',\n",
       " '@',\n",
       " '[',\n",
       " '\\\\',\n",
       " ']',\n",
       " '^',\n",
       " '_',\n",
       " '`',\n",
       " '~',\n",
       " 'Â¡',\n",
       " 'Â¢',\n",
       " 'Â£',\n",
       " 'Â¥',\n",
       " 'Â¦',\n",
       " 'Â¨',\n",
       " 'Â®',\n",
       " 'Â¯',\n",
       " 'Â°',\n",
       " 'Â±',\n",
       " 'Â´',\n",
       " 'Âµ',\n",
       " 'Â·',\n",
       " 'Â¸',\n",
       " 'Â¹',\n",
       " 'Âº',\n",
       " 'Â»',\n",
       " 'Â½',\n",
       " 'Â¾',\n",
       " 'Â¿',\n",
       " 'Ã€',\n",
       " 'Ã',\n",
       " 'Ã‚',\n",
       " 'Ã‡',\n",
       " 'ÃŠ',\n",
       " 'Ã',\n",
       " 'Ã',\n",
       " 'Ã‘',\n",
       " 'Ã™',\n",
       " 'Ã›',\n",
       " 'Ã¢',\n",
       " 'Ã¦',\n",
       " 'Ã§',\n",
       " 'Ã±',\n",
       " 'Ã³',\n",
       " 'Ã¶',\n",
       " 'Ã¸',\n",
       " 'Ã»',\n",
       " 'Ë',\n",
       " 'â€“',\n",
       " 'â€”',\n",
       " 'â€•',\n",
       " 'â€˜',\n",
       " 'â€™',\n",
       " 'â€œ',\n",
       " 'â€',\n",
       " 'â€¦',\n",
       " 'â„ƒ',\n",
       " 'âˆ¼'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set(find_special_char(dec_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ea1a7317",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence, s_token=False, e_token=False):\n",
    "\n",
    "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "    sentence = re.sub(r\"[^ê°€-í£0-9a-zA-Z?.!,]+\", \" \", sentence)\n",
    "\n",
    "    sentence = sentence.strip()\n",
    "\n",
    "    if s_token:\n",
    "        sentence = '<start> ' + sentence\n",
    "\n",
    "    if e_token:\n",
    "        sentence += ' <end>'\n",
    "    \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "30a03323",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… í† í¬ë‚˜ì´ì €ê°€ ì •ìƒì ìœ¼ë¡œ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import tensorflow as tf\n",
    "\n",
    "# âœ… JSONì—ì„œ `word_index`ë§Œ ë¡œë“œ\n",
    "def load_word_index(json_path):\n",
    "    with open(json_path, 'r', encoding='utf-8') as f:\n",
    "        word_index = json.load(f)  # âœ… JSON íŒŒì¼ ë¡œë“œ\n",
    "    return word_index\n",
    "\n",
    "# âœ… `word_index`ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìƒˆë¡œìš´ `Tokenizer` ìƒì„±\n",
    "def create_tokenizer_from_word_index(word_index):\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')  # âœ… í•„í„°ë§ ì—†ì´ ìƒì„±\n",
    "    tokenizer.word_index = word_index  # âœ… ê¸°ì¡´ `word_index` ì ìš©\n",
    "    tokenizer.index_word = {v: k for k, v in word_index.items()}  # âœ… `index_word` ìƒì„±\n",
    "    return tokenizer\n",
    "\n",
    "# ğŸ”¹ í•œêµ­ì–´ & ì˜ì–´ `word_index` ë¡œë“œ\n",
    "enc_word_index = load_word_index(\"data/enc_tokenizer.json\")  # âœ… Mecabìœ¼ë¡œ ë§Œë“  word_index\n",
    "dec_word_index = load_word_index(\"data/dec_tokenizer.json\")  # âœ… ê³µë°± ë‹¨ìœ„ë¡œ splití•œ word_index\n",
    "\n",
    "# ğŸ”¹ `word_index` ê¸°ë°˜ `Tokenizer` ìƒì„±\n",
    "enc_tokenizer = create_tokenizer_from_word_index(enc_word_index)\n",
    "dec_tokenizer = create_tokenizer_from_word_index(dec_word_index)\n",
    "\n",
    "print(\"âœ… í† í¬ë‚˜ì´ì €ê°€ ì •ìƒì ìœ¼ë¡œ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c2c464f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… enc_train shape: (30000, 30)\n",
      "âœ… dec_train shape: (30000, 40)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# âœ… í† í°í™”ëœ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¤ê¸° (ì´ë¯¸ í† í°í™”ëœ ë°ì´í„° ì‚¬ìš©)\n",
    "data = np.load(\"data/tokenized_data.npz\")\n",
    "\n",
    "# âœ… ê¸°ì¡´ì˜ í† í°í™”ëœ ë°ì´í„°\n",
    "enc_train = data['enc_corpus']  # í•œêµ­ì–´ (ì…ë ¥ ë°ì´í„°)\n",
    "dec_train = data['dec_corpus']  # ì˜ì–´ (ì¶œë ¥ ë°ì´í„°)\n",
    "\n",
    "# âœ… `maxlen` ì„¤ì • (enc=30, dec=40)\n",
    "ENC_MAXLEN = 30\n",
    "DEC_MAXLEN = 40\n",
    "\n",
    "# âœ… `pad_sequences()`ë¥¼ ì´ìš©í•´ íŒ¨ë”© ì ìš©\n",
    "enc_train = tf.keras.preprocessing.sequence.pad_sequences(enc_train, maxlen=ENC_MAXLEN, padding='post')\n",
    "dec_train = tf.keras.preprocessing.sequence.pad_sequences(dec_train, maxlen=DEC_MAXLEN, padding='post')\n",
    "\n",
    "print(f\"âœ… enc_train shape: {enc_train.shape}\")  # (ìƒ˜í”Œ ìˆ˜, 30)\n",
    "print(f\"âœ… dec_train shape: {dec_train.shape}\")  # (ìƒ˜í”Œ ìˆ˜, 40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e269605f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "72ef302a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "í•œêµ­ì–´: ì œ 23ì°¨ ì—°ë¡€ ì»´ë±ìŠ¤ ë°•ëŒíšŒì˜ ê°œíšŒì‚¬ë¥¼ í•œ ì¼€ì´ì¸ ëŠ” 2ë…„ì—¬ì „ ê¸°ìˆ  ì‚°ì—…ì˜ ê±°í’ˆì´ ë¶•ê´´ëœ ì´í›„ì— ì²¨ë‹¨ ê¸°ìˆ ì— ëŒ€í•´ ë¶€ì •ì ì¸ ì¸ì‹ì´ ìˆë‹¤ê³  ë§í–ˆë‹¤ .\n",
      "ì˜ì–´: <start> Gates , who opened the 23rd annual Comdex trade show , said there was a negative perception of high tech following the collapse of the tech bubble about two years ago . <end>\n"
     ]
    }
   ],
   "source": [
    "# enc_corpus = []\n",
    "# dec_corpus = []\n",
    "\n",
    "# num_examples = 50000\n",
    "\n",
    "# for kor in enc_data[:num_examples]:\n",
    "#     enc_corpus.append(preprocess_sentence(kor))\n",
    "    \n",
    "# for eng in dec_data[:num_examples]:\n",
    "#     dec_corpus.append(preprocess_sentence(eng, s_token=True, e_token=True))\n",
    "\n",
    "# print(\"í•œêµ­ì–´:\", enc_corpus[100])\n",
    "# print(\"ì˜ì–´:\", dec_corpus[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8e948ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import MeCab\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eac1bf50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /aiffel/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# nltk.download('punkt')\n",
    "# mecab = MeCab.Tagger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "906230c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def tokenize(corpus, lang='ko'):\n",
    "#     tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
    "\n",
    "#     tokenized_corpus = []\n",
    "    \n",
    "#     for sentence in corpus:\n",
    "#         if lang == 'ko':  # í•œêµ­ì–´ í…ìŠ¤íŠ¸ì¼ ê²½ìš° MeCab ì‚¬ìš©\n",
    "#             parsed = mecab.parse(sentence)\n",
    "#             tokens = [line.split('\\t')[0] for line in parsed.splitlines() if line]\n",
    "#         elif lang == 'en':  # ì˜ì–´ í…ìŠ¤íŠ¸ì¼ ê²½ìš° NLTK ì‚¬ìš©\n",
    "#             tokens = nltk.word_tokenize(sentence)\n",
    "#         else:\n",
    "#             raise ValueError(\"Language should be either 'ko' or 'en'.\")\n",
    "\n",
    "#         # ë¦¬ìŠ¤íŠ¸ì— í† í° ì¶”ê°€\n",
    "#         tokenized_corpus.append(' '.join(tokens))\n",
    "    \n",
    "#     tokenizer.fit_on_texts(tokenized_corpus)  # í† í°ì„ ê¸°ë°˜ìœ¼ë¡œ Tokenizer í•™ìŠµ\n",
    "    \n",
    "#     # íŠ¹ìˆ˜ í† í° ì¶”ê°€\n",
    "#     tokenizer.word_index['<start>'] = len(tokenizer.word_index) + 1\n",
    "#     tokenizer.word_index['<end>'] = len(tokenizer.word_index) + 2\n",
    "    \n",
    "#     tensor = tokenizer.texts_to_sequences(tokenized_corpus)  # í…ìŠ¤íŠ¸ë¥¼ ì‹œí€€ìŠ¤ë¡œ ë³€í™˜\n",
    "#     tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')  # ì‹œí€€ìŠ¤ ê¸¸ì´ë¥¼ ë™ì¼í•˜ê²Œ ë§ì¶”ê¸°\n",
    "\n",
    "#     return tensor, tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "090e6aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# enc_corpus, enc_tokenizer = tokenize(enc_corpus)\n",
    "# dec_corpus, dec_tokenizer = tokenize(dec_corpus, lang='en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f79ea368",
   "metadata": {},
   "outputs": [],
   "source": [
    "# enc_train, enc_val, dec_train, dec_val = train_test_split(enc_corpus, dec_corpus, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8a2b4056",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.w_dec = tf.keras.layers.Dense(units)\n",
    "        self.w_enc = tf.keras.layers.Dense(units)\n",
    "        self.w_com = tf.keras.layers.Dense(1)\n",
    "    \n",
    "    def call(self, h_enc, h_dec):\n",
    "        # h_enc shape: [batch x length x units]\n",
    "        # h_dec shape: [batch x units]\n",
    "\n",
    "        h_enc = self.w_enc(h_enc)\n",
    "        h_dec = tf.expand_dims(h_dec, 1)\n",
    "        h_dec = self.w_dec(h_dec)\n",
    "\n",
    "        score = self.w_com(tf.nn.tanh(h_dec + h_enc))\n",
    "        \n",
    "        attn = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "        context_vec = attn * h_enc\n",
    "        context_vec = tf.reduce_sum(context_vec, axis=1)\n",
    "\n",
    "        return context_vec, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "13ae3b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(enc_units,\n",
    "                                       return_sequences=True)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.gru(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9abdd8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(dec_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True)\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "        self.attention = BahdanauAttention(self.dec_units)\n",
    "\n",
    "    def call(self, x, h_dec, enc_out):\n",
    "        context_vec, attn = self.attention(enc_out, h_dec)\n",
    "\n",
    "        out = self.embedding(x)\n",
    "        out = tf.concat([tf.expand_dims(context_vec, 1), out], axis=-1)\n",
    "        \n",
    "        out, h_dec = self.gru(out)\n",
    "        out = tf.reshape(out, (-1, out.shape[2]))\n",
    "        out = self.fc(out)\n",
    "\n",
    "        return out, h_dec, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "709e201e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder Output: (32, 30, 512)\n",
      "Decoder Output: (32, 30370)\n",
      "Decoder Hidden State: (32, 512)\n",
      "Attention: (32, 30, 1)\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE     = 32\n",
    "SRC_VOCAB_SIZE = len(enc_tokenizer.index_word) + 1\n",
    "TGT_VOCAB_SIZE = len(dec_tokenizer.index_word) + 1\n",
    "\n",
    "units         = 512\n",
    "embedding_dim = 256\n",
    "\n",
    "encoder = Encoder(SRC_VOCAB_SIZE, embedding_dim, units)\n",
    "decoder = Decoder(TGT_VOCAB_SIZE, embedding_dim, units)\n",
    "\n",
    "# sample input\n",
    "sequence_len = 30\n",
    "\n",
    "sample_enc = tf.random.uniform((BATCH_SIZE, sequence_len))\n",
    "sample_output = encoder(sample_enc)\n",
    "\n",
    "print ('Encoder Output:', sample_output.shape)\n",
    "\n",
    "sample_state = tf.random.uniform((BATCH_SIZE, units))\n",
    "\n",
    "sample_logits, h_dec, attn = decoder(tf.random.uniform((BATCH_SIZE, 1)),\n",
    "                                     sample_state, sample_output)\n",
    "\n",
    "print ('Decoder Output:', sample_logits.shape)\n",
    "print ('Decoder Hidden State:', h_dec.shape)\n",
    "print ('Attention:', attn.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6f35dd5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss = loss_object(real, pred)\n",
    "    \n",
    "    mask = tf.cast(mask, dtype=loss.dtype)\n",
    "    loss *= mask\n",
    "    \n",
    "    return tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d32365ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì²´í¬í¬ì¸íŠ¸ ì„¤ì • (ë² ìŠ¤íŠ¸ ëª¨ë¸ ì €ì¥)\n",
    "def create_checkpoint_manager(encoder, decoder, optimizer, checkpoint_dir='./checkpoints'):\n",
    "    checkpoint = tf.train.Checkpoint(optimizer=optimizer, encoder=encoder, decoder=decoder)\n",
    "    manager = tf.train.CheckpointManager(checkpoint, checkpoint_dir, max_to_keep=1)\n",
    "    return checkpoint, manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "85cc98f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1aa8a7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(src, tgt, encoder, decoder, optimizer, dec_tok):\n",
    "    bsz = src.shape[0]\n",
    "    loss = 0\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        enc_out = encoder(src)\n",
    "        h_dec = enc_out[:, -1]\n",
    "        \n",
    "        dec_src = tf.expand_dims([dec_tok.word_index['<start>']] * bsz, 1)\n",
    "\n",
    "        for t in range(1, tgt.shape[1]):\n",
    "            pred, h_dec, _ = decoder(dec_src, h_dec, enc_out)\n",
    "\n",
    "            loss += loss_function(tgt[:, t], pred)\n",
    "            dec_src = tf.expand_dims(tgt[:, t], 1)\n",
    "        \n",
    "    batch_loss = (loss / int(tgt.shape[1]))\n",
    "\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "    return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a53bedbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì²´í¬í¬ì¸íŠ¸ ì„¤ì •\n",
    "checkpoint_dir = './checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer, encoder=encoder, decoder=decoder)\n",
    "checkpoint_manager = tf.train.CheckpointManager(checkpoint, checkpoint_dir, max_to_keep=3)\n",
    "\n",
    "best_val_loss = float('inf')  # ì´ˆê¸° ê²€ì¦ ì†ì‹¤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "04939175",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval_step í•¨ìˆ˜ ì •ì˜\n",
    "@tf.function\n",
    "def eval_step(src, tgt, encoder, decoder, dec_tok):\n",
    "    bsz = src.shape[0]\n",
    "    loss = 0\n",
    "\n",
    "    enc_out = encoder(src)\n",
    "\n",
    "    h_dec = enc_out[:, -1]\n",
    "    \n",
    "    dec_src = tf.expand_dims([dec_tok.word_index['<start>']] * bsz, 1)\n",
    "\n",
    "    for t in range(1, tgt.shape[1]):\n",
    "        pred, h_dec, _ = decoder(dec_src, h_dec, enc_out)\n",
    "\n",
    "        loss += loss_function(tgt[:, t], pred)\n",
    "        dec_src = tf.expand_dims(tgt[:, t], 1)\n",
    "        \n",
    "    batch_loss = (loss / int(tgt.shape[1]))\n",
    "    \n",
    "    return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e140d086",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 938/938 [03:22<00:00,  4.63it/s, loss=0.0385]  \n",
      "Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 938/938 [02:14<00:00,  6.98it/s, loss=0.0330]\n",
      "Epoch 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 938/938 [02:14<00:00,  6.98it/s, loss=0.0328]\n",
      "Epoch 4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 938/938 [02:14<00:00,  6.97it/s, loss=0.0326]\n",
      "Epoch 5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 938/938 [02:14<00:00,  6.97it/s, loss=0.0326]\n",
      "Epoch 6: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 938/938 [02:14<00:00,  6.98it/s, loss=0.0327]\n",
      "Epoch 7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 938/938 [02:14<00:00,  6.98it/s, loss=0.0327]\n",
      "Epoch 8: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 938/938 [02:14<00:00,  6.98it/s, loss=0.0326]\n",
      "Epoch 9: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 938/938 [02:14<00:00,  6.98it/s, loss=0.0327]\n",
      "Epoch 10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 938/938 [02:14<00:00,  6.98it/s, loss=0.0327]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… í›ˆë ¨ ì™„ë£Œ! ëª¨ë¸ì´ ì„±ê³µì ìœ¼ë¡œ í•™ìŠµë˜ì—ˆìŠµë‹ˆë‹¤.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "EPOCHS = 10  # í•™ìŠµ ë°˜ë³µ íšŸìˆ˜\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0  # ì—í¬í¬ë³„ ì†ì‹¤ ì´ˆê¸°í™”\n",
    "    \n",
    "    # ğŸ”¹ ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì„ê¸°\n",
    "    idx_list = list(range(0, enc_train.shape[0], BATCH_SIZE))\n",
    "    random.shuffle(idx_list)\n",
    "    \n",
    "    # ğŸ”¹ tqdmì„ ì‚¬ìš©í•œ í•™ìŠµ ì§„í–‰ í‘œì‹œ\n",
    "    t = tqdm(idx_list, desc=f\"Epoch {epoch + 1}\")\n",
    "\n",
    "    # ğŸ”¹ í•™ìŠµ ì§„í–‰\n",
    "    for (batch, idx) in enumerate(t):\n",
    "        batch_loss = train_step(enc_train[idx:idx+BATCH_SIZE],  # âœ… ì…ë ¥ (í•œêµ­ì–´)\n",
    "                                dec_train[idx:idx+BATCH_SIZE],  # âœ… ì¶œë ¥ (ì˜ì–´)\n",
    "                                encoder,\n",
    "                                decoder,\n",
    "                                optimizer,\n",
    "                                dec_tokenizer)\n",
    "    \n",
    "        total_loss += batch_loss  # ì†ì‹¤ê°’ ëˆ„ì \n",
    "        t.set_postfix(loss=f\"{total_loss.numpy() / (batch + 1):.4f}\")  # ì§„í–‰ ìƒí™© ì¶œë ¥\n",
    "\n",
    "print(\"âœ… í›ˆë ¨ ì™„ë£Œ! ëª¨ë¸ì´ ì„±ê³µì ìœ¼ë¡œ í•™ìŠµë˜ì—ˆìŠµë‹ˆë‹¤.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0b018aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(sentence, encoder, decoder):\n",
    "    attention = np.zeros((dec_train.shape[-1], enc_train.shape[-1]))\n",
    "    \n",
    "    sentence = preprocess_sentence(sentence)\n",
    "    inputs = enc_tokenizer.texts_to_sequences([sentence.split()])\n",
    "    inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs,\n",
    "                                                           maxlen=enc_train.shape[-1],\n",
    "                                                           padding='post')\n",
    "\n",
    "    result = ''\n",
    "\n",
    "    enc_out = encoder(inputs)\n",
    "\n",
    "    dec_hidden = enc_out[:, -1]\n",
    "    dec_input = tf.expand_dims([dec_tokenizer.word_index['<start>']], 0)\n",
    "\n",
    "    for t in range(dec_train.shape[-1]):\n",
    "        predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
    "                                                             dec_hidden,\n",
    "                                                             enc_out)\n",
    "\n",
    "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "        attention[t] = attention_weights.numpy()\n",
    "\n",
    "        predicted_id = \\\n",
    "        tf.argmax(tf.math.softmax(predictions, axis=-1)[0]).numpy()\n",
    "\n",
    "        result += dec_tokenizer.index_word[predicted_id] + ' '\n",
    "\n",
    "        if dec_tokenizer.index_word[predicted_id] == '<end>':\n",
    "            return result, sentence, attention\n",
    "\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    return result, sentence, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "00cdfd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_attention(attention, sentence, predicted_sentence):\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.matshow(attention, cmap='viridis')\n",
    "\n",
    "    fontdict = {'fontsize': 14}\n",
    "\n",
    "    ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
    "    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
    "\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1956fc4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentence, encoder, decoder):\n",
    "    result, sentence, attention = evaluate(sentence, encoder, decoder)\n",
    "\n",
    "    print('Input: %s' % (sentence))\n",
    "    print('Predicted translation: {}'.format(result))\n",
    "    \n",
    "    attention = attention[:len(result.split()), :len(sentence.split())]\n",
    "    plot_attention(attention, sentence.split(), result.split(' '))\n",
    "\n",
    "\n",
    "translate(\"Can I have some coffee?\", encoder, decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a94cf77c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì…ë ¥ ë¬¸ì¥: ì˜¤ë°”ë§ˆëŠ” ëŒ€í†µë ¹ì´ë‹¤ .\n",
      "ë²ˆì—­ ê²°ê³¼: . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . \n",
      "\n",
      "ì…ë ¥ ë¬¸ì¥: ì‹œë¯¼ë“¤ì€ ë„ì‹œ ì†ì— ì‚°ë‹¤ .\n",
      "ë²ˆì—­ ê²°ê³¼: . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . \n",
      "\n",
      "ì…ë ¥ ë¬¸ì¥: ì»¤í”¼ëŠ” í•„ìš” ì—†ë‹¤ .\n",
      "ë²ˆì—­ ê²°ê³¼: . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . \n",
      "\n",
      "ì…ë ¥ ë¬¸ì¥: ì¼ê³± ëª…ì˜ ì‚¬ë§ìê°€ ë°œìƒí–ˆë‹¤ .\n",
      "ë²ˆì—­ ê²°ê³¼: . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "\n",
    "# âœ… ë²ˆì—­í•  ë¬¸ì¥ ë¦¬ìŠ¤íŠ¸\n",
    "input_sentences = [\n",
    "    \"ì˜¤ë°”ë§ˆëŠ” ëŒ€í†µë ¹ì´ë‹¤.\",\n",
    "    \"ì‹œë¯¼ë“¤ì€ ë„ì‹œ ì†ì— ì‚°ë‹¤.\",\n",
    "    \"ì»¤í”¼ëŠ” í•„ìš” ì—†ë‹¤.\",\n",
    "    \"ì¼ê³± ëª…ì˜ ì‚¬ë§ìê°€ ë°œìƒí–ˆë‹¤.\"\n",
    "]\n",
    "\n",
    "# âœ… ê° ë¬¸ì¥ì— ëŒ€í•´ ë²ˆì—­ ì‹¤í–‰\n",
    "for sentence in input_sentences:\n",
    "    translated_sentence, processed_input, attention = evaluate(sentence, encoder, decoder)\n",
    "\n",
    "    # âœ… ë²ˆì—­ ê²°ê³¼ ì¶œë ¥\n",
    "    print(f\"ì…ë ¥ ë¬¸ì¥: {processed_input}\")\n",
    "    print(f\"ë²ˆì—­ ê²°ê³¼: {translated_sentence}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d9d69861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•œêµ­ì–´ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ í•¨ìˆ˜\n",
    "def kor_preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip()\n",
    "\n",
    "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "    sentence = re.sub(r\"[^ê°€-í£a-zA-Z0-9?.!,]+\", \" \", sentence)\n",
    "\n",
    "    sentence = sentence.strip()\n",
    "    \n",
    "    return sentence\n",
    "\n",
    "\n",
    "# ì˜ì–´ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ í•¨ìˆ˜\n",
    "def eng_preprocess_sentence(sentence, s_token=False, e_token=False):\n",
    "    sentence = sentence.lower().strip()\n",
    "\n",
    "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,]+\", \" \", sentence)\n",
    "\n",
    "    sentence = sentence.strip()\n",
    "\n",
    "    if s_token:\n",
    "        sentence = '<start> ' + sentence\n",
    "\n",
    "    if e_token:\n",
    "        sentence += ' <end>'\n",
    "    \n",
    "    return sentence\n",
    "\n",
    "def evaluate(sentence, encoder, decoder):\n",
    "    attention = np.zeros((dec_train.shape[-1], enc_train.shape[-1]))\n",
    "    \n",
    "    sentence = kor_preprocess_sentence(sentence)\n",
    "    inputs = enc_tokenizer.texts_to_sequences([sentence.split()])\n",
    "    inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs,\n",
    "                                                           maxlen=enc_train.shape[-1],\n",
    "                                                           padding='post')\n",
    "\n",
    "    result = ''\n",
    "\n",
    "    enc_out = encoder(inputs)\n",
    "\n",
    "    dec_hidden = enc_out[:, -1]\n",
    "    dec_input = tf.expand_dims([dec_tokenizer.word_index['<start>']], 0)\n",
    "\n",
    "    for t in range(dec_train.shape[-1]):\n",
    "        predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
    "                                                             dec_hidden,\n",
    "                                                             enc_out)\n",
    "\n",
    "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "        attention[t] = attention_weights.numpy()\n",
    "\n",
    "        predicted_id = \\\n",
    "        tf.argmax(tf.math.softmax(predictions, axis=-1)[0]).numpy()\n",
    "\n",
    "        result += dec_tokenizer.index_word[predicted_id] + ' '\n",
    "\n",
    "        if dec_tokenizer.index_word[predicted_id] == '<end>':\n",
    "            return result, sentence, attention\n",
    "\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    return result, sentence, attention\n",
    "\n",
    "\n",
    "def plot_attention(attention, sentence, predicted_sentence):\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.matshow(attention, cmap='viridis')\n",
    "\n",
    "    fontdict = {'fontsize': 14}\n",
    "\n",
    "    ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
    "    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
    "\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "642172da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì…ë ¥ ë¬¸ì¥: ì˜¤ë°”ë§ˆëŠ” ëŒ€í†µë ¹ì´ë‹¤ .\n",
      "ë²ˆì—­ ê²°ê³¼: . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . \n",
      "\n",
      "ì…ë ¥ ë¬¸ì¥: ì‹œë¯¼ë“¤ì€ ë„ì‹œ ì†ì— ì‚°ë‹¤ .\n",
      "ë²ˆì—­ ê²°ê³¼: . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . \n",
      "\n",
      "ì…ë ¥ ë¬¸ì¥: ì»¤í”¼ëŠ” í•„ìš” ì—†ë‹¤ .\n",
      "ë²ˆì—­ ê²°ê³¼: . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . \n",
      "\n",
      "ì…ë ¥ ë¬¸ì¥: ì¼ê³± ëª…ì˜ ì‚¬ë§ìê°€ ë°œìƒí–ˆë‹¤ .\n",
      "ë²ˆì—­ ê²°ê³¼: . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "\n",
    "# âœ… ë²ˆì—­í•  ë¬¸ì¥ ë¦¬ìŠ¤íŠ¸\n",
    "input_sentences = [\n",
    "    \"ì˜¤ë°”ë§ˆëŠ” ëŒ€í†µë ¹ì´ë‹¤.\",\n",
    "    \"ì‹œë¯¼ë“¤ì€ ë„ì‹œ ì†ì— ì‚°ë‹¤.\",\n",
    "    \"ì»¤í”¼ëŠ” í•„ìš” ì—†ë‹¤.\",\n",
    "    \"ì¼ê³± ëª…ì˜ ì‚¬ë§ìê°€ ë°œìƒí–ˆë‹¤.\"\n",
    "]\n",
    "\n",
    "# âœ… ê° ë¬¸ì¥ì— ëŒ€í•´ ë²ˆì—­ ì‹¤í–‰\n",
    "for sentence in input_sentences:\n",
    "    translated_sentence, processed_input, attention = evaluate(sentence, encoder, decoder)\n",
    "\n",
    "    # âœ… ë²ˆì—­ ê²°ê³¼ ì¶œë ¥\n",
    "    print(f\"ì…ë ¥ ë¬¸ì¥: {processed_input}\")\n",
    "    print(f\"ë²ˆì—­ ê²°ê³¼: {translated_sentence}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30328ac6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
